{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Wide and deep architect has been proven as one of deep learning applications combining memorization and generatlization in areas such as search and recommendation. Google released its [wide&deep learning](https://ai.googleblog.com/2016/06/wide-deep-learning-better-together-with.html) in 2016. \n",
    "\n",
    "* wide part: helps to memorize the past behaviour for specific choice\n",
    "* deep part: embed into low dimension, help to discover new user, product combinations\n",
    "\n",
    "Later, on top of wide & deep learning, [deepfm](https://arxiv.org/abs/1703.04247) was developed combining DNN model and Factorization machines, to furthur address the interactions among the features. \n",
    "\n",
    "## wide & deep model\n",
    "![wide&deep learning](https://1.bp.blogspot.com/-Dw1mB9am1l8/V3MgtOzp3uI/AAAAAAAABGs/mP-3nZQCjWwdk6qCa5WraSpK8A7rSPj3ACLcB/s640/image04.png)\n",
    "## deepFM model\n",
    "![deepfm learning](https://www.researchgate.net/profile/Huifeng_Guo/publication/318829508/figure/fig1/AS:522607722467328@1501610798143/Wide-deep-architecture-of-DeepFM-The-wide-and-deep-component-share-the-same-input-raw.png)\n",
    "\n",
    "## Comparison\n",
    "In wide part of wide & deep learning, it is a logistic regression, which requires a lot of manual feature engineering efforts to generate the large-scale feature set for wide part. While the deepfm model instead has a shared embeded layers for both deep and fm parts, dot product between embeded features  also address the interactions.\n",
    "\n",
    "## deepFM model in details\n",
    "* 1st order factorization machines (summation of all embed layers)\n",
    "    + numeric features with shape (None, 1) => dense layer => map to shape (None, 1)\n",
    "    + categorical features (single level) with shape (None,1) => embedding layer (latent_dim = 1) => map to shape (None, 1)\n",
    "    + categorical features (multi level) with shape (None,L) => embedding layer (latent_dim = 1) => map to shape (None, L)\n",
    "    + output will summation of all embeded features, result in a tensor with shape (None, 1)\n",
    "* 2nd order factorization machines (summation of dot product between embed layers)\n",
    "    + numeric features => dense layer => map to shape (None, 1, k)\n",
    "    + categorical features (single level) => embedding layer (latent_dim = k) => map to shape (None, 1, k)\n",
    "    + categorical features (multi level) with shape (None,L) => embedding layer (latent_dim = k) => map to shape (None, L, k)\n",
    "    + shared embed layer will be the concatenated layers of all embeded features\n",
    "    + shared embed layer => dot layer => 2nd order of fm part\n",
    "* deep part (DNN model on shared embed layers)\n",
    "    + shared embed layer => dense layer => deep part\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess data\n",
    "\n",
    "The dataset used to implement deepfm is movieLens(ml-1m) data.    \n",
    "To add more features to the ratings.csv, I joined the user features and movies features.\n",
    "The features used are as below:\n",
    "* numeric feature: user_fea3\n",
    "* categorical feature (single level): uid, mid\n",
    "* categorical feature (multi level): movie_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== rating.dat ======\n",
      "   uid   mid  rating  timestamp\n",
      "0    1  1193       5  978300760\n",
      "1    1   661       3  978302109\n",
      "2    1   914       3  978301968\n",
      "3    1  3408       4  978300275\n",
      "4    1  2355       5  978824291\n",
      "===== movies.dat ======\n",
      "   mid                          movie_name                   movie_genre\n",
      "0    1                    Toy Story (1995)   Animation|Children's|Comedy\n",
      "1    2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
      "2    3             Grumpier Old Men (1995)                Comedy|Romance\n",
      "3    4            Waiting to Exhale (1995)                  Comedy|Drama\n",
      "4    5  Father of the Bride Part II (1995)                        Comedy\n",
      "====== users.dat ======\n",
      "   uid user_fea1  user_fea2  user_fea3 user_fea4\n",
      "0    1         F          1         10     48067\n",
      "1    2         M         56         16     70072\n",
      "2    3         M         25         15     55117\n",
      "3    4         M         45          7     02460\n",
      "4    5         M         25         20     55455\n",
      "   mid                          movie_name movie_genre\n",
      "0    1                    Toy Story (1995)   [9, 2, 0]\n",
      "1    2                      Jumanji (1995)   [7, 9, 0]\n",
      "2    3             Grumpier Old Men (1995)   [2, 5, 0]\n",
      "3    4            Waiting to Exhale (1995)   [2, 1, 0]\n",
      "4    5  Father of the Bride Part II (1995)   [2, 0, 0]\n",
      "====== preprocessed data =======\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>mid</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>movie_name</th>\n",
       "      <th>movie_genre</th>\n",
       "      <th>user_fea1</th>\n",
       "      <th>user_fea2</th>\n",
       "      <th>user_fea3</th>\n",
       "      <th>user_fea4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "      <td>One Flew Over the Cuckoo's Nest (1975)</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "      <td>James and the Giant Peach (1996)</td>\n",
       "      <td>[9, 13, 0]</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "      <td>My Fair Lady (1964)</td>\n",
       "      <td>[13, 5, 0]</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "      <td>Erin Brockovich (2000)</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "      <td>Bug's Life, A (1998)</td>\n",
       "      <td>[9, 2, 0]</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid   mid  rating  timestamp                              movie_name  \\\n",
       "0    1  1193       5  978300760  One Flew Over the Cuckoo's Nest (1975)   \n",
       "1    1   661       3  978302109        James and the Giant Peach (1996)   \n",
       "2    1   914       3  978301968                     My Fair Lady (1964)   \n",
       "3    1  3408       4  978300275                  Erin Brockovich (2000)   \n",
       "4    1  2355       5  978824291                    Bug's Life, A (1998)   \n",
       "\n",
       "  movie_genre user_fea1  user_fea2  user_fea3 user_fea4  \n",
       "0   [1, 0, 0]         F          1         10     48067  \n",
       "1  [9, 13, 0]         F          1         10     48067  \n",
       "2  [13, 5, 0]         F          1         10     48067  \n",
       "3   [1, 0, 0]         F          1         10     48067  \n",
       "4   [9, 2, 0]         F          1         10     48067  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_ratings():\n",
    "    COL_NAME = ['uid','mid','rating','timestamp']\n",
    "    df = pd.read_csv('./dataset/ml-1m/ratings.dat',sep='::', header=None, engine='python', names=COL_NAME)\n",
    "    return df\n",
    "\n",
    "def load_movies():\n",
    "    COL_NAME = ['mid','movie_name','movie_genre']\n",
    "    df = pd.read_csv('./dataset/ml-1m/movies.dat',sep='::', header=None, engine='python', names=COL_NAME)\n",
    "    return df\n",
    "\n",
    "def load_users():\n",
    "    COL_NAME = ['uid','user_fea1','user_fea2','user_fea3','user_fea4']\n",
    "    df = pd.read_csv('./dataset/ml-1m/users.dat',sep='::', header=None, engine='python', names=COL_NAME)\n",
    "    return df\n",
    "\n",
    "def text2seq(text, n_genre):\n",
    "    \"\"\" using tokenizer to encoded the multi-level categorical feature\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(lower=True, split='|',filters='', num_words=n_genre)\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    seq = tokenizer.texts_to_sequences(text)\n",
    "    seq = pad_sequences(seq, maxlen=3,padding='post')\n",
    "    return seq\n",
    "\n",
    "n_genre = 15\n",
    "\n",
    "ratings = load_ratings()\n",
    "movies = load_movies()\n",
    "users = load_users()\n",
    "\n",
    "print(\"====== rating.dat ======\")\n",
    "print(ratings.head())\n",
    "print(\"===== movies.dat ======\")\n",
    "print(movies.head())\n",
    "print(\"====== users.dat ======\")\n",
    "print(users.head())\n",
    "\n",
    "movies['movie_genre'] = text2seq(movies.movie_genre.values, n_genre=n_genre).tolist()\n",
    "\n",
    "ratings = ratings.join(movies.set_index('mid'), on = 'mid', how = 'left')\n",
    "ratings = ratings.join(users.set_index('uid'), on = 'uid', how = 'left')\n",
    "print(\"====== preprocessed data =======\")\n",
    "(ratings.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct model\n",
    "\n",
    "* define input layers\n",
    "\n",
    "``` python\n",
    "# numerica features\n",
    "fea3_input = Input((1,), name = 'input_fea3')\n",
    "num_inputs = [fea4_input]\n",
    "# single level categorical features\n",
    "uid_input = Input((1,), name = 'input_uid')\n",
    "mid_input = Input((1,), name= 'input_mid')\n",
    "cat_sl_inputs = [uid_input, mid_input]\n",
    "\n",
    "# multi level categorical features (with 3 genres at most)\n",
    "genre_input = Input((3,), name = 'input_genre')\n",
    "cat_ml_inputs = [genre_input]\n",
    "\n",
    "inputs = num_inputs + cat_sl_inputs + cat_ml_inputs\n",
    "```\n",
    "\n",
    "* 1st order factorization machines\n",
    "\n",
    "```python\n",
    "# all tensors are reshape to (None, 1)\n",
    "num_dense_1d = [Dense(1, name = 'num_dense_1d_fea4')(fea4_input)]\n",
    "cat_sl_embed_1d = [Embedding(n_uid + 1, 1, name = 'cat_embed_1d_uid')(uid_input),\n",
    "                    Embedding(n_mid + 1, 1, name = 'cat_embed_1d_mid')(mid_input)]\n",
    "cat_ml_embed_1d = [Embedding(n_genre + 1, 1, name = 'cat_embed_1d_genre')(genre_input)]\n",
    "\n",
    "cat_sl_embed_1d = [Reshape((1,))(i) for i in cat_sl_embed_1d]\n",
    "cat_ml_embed_1d = [Flatten()(i) for i in cat_ml_embed_1d]\n",
    "\n",
    "# add all tensors\n",
    "y_fm_1d = Add(name = 'fm_1d_output')(num_dense_1d + cat_sl_embed_1d + cat_ml_embed_1d)\n",
    "```\n",
    "![title](./image/fm_model_1d.png)\n",
    "\n",
    "* 2nd order factorization machines\n",
    "the 2nd order fm can be simplified, using \n",
    "\\begin{equation*}\n",
    "\\sum{x_ix_j} = \\frac{1}{2} \\left((\\sum{x})^2 - \\sum({x}^2)\\right)\n",
    "\\end{equation*}\n",
    "\n",
    "```python\n",
    "# reshape all tensors to (None, k)\n",
    "num_dense_2d = [Dense(k, name = 'num_dense_2d_fea4')(fea4_input)]\n",
    "cat_sl_embed_2d = [Embedding(n_uid + 1, k, name = 'cat_embed_2d_uid')(uid_input), \n",
    "                   Embedding(n_mid + 1, k, name = 'cat_embed_2d_mid')(mid_input)]\n",
    "cat_ml_embed_2d = [Embedding(n_genre + 1, k, name = 'cat_embed_2d_genre')(genre_input)]\n",
    "cat_ml_embed_2d = [Lambda(lambda x: K.mean(x, axis=1), name = 'embed_2d_mean')(i) for i in cat_ml_embed_2d]\n",
    "\n",
    "num_dense_2d = [Reshape((1,k))(i) for i in num_dense_2d]\n",
    "cat_ml_embed_2d = [Reshape((1,k))(i) for i in cat_ml_embed_2d]\n",
    "\n",
    "embed_2d = Concatenate(axis=1, name = 'concat_embed_2d')(num_dense_2d + cat_sl_embed_2d + cat_ml_embed_2d)\n",
    "\n",
    "# calcuate the interactions by simplication\n",
    "# sum of (x1*x2) = sum of (0.5*[(xi)^2 - (xi^2)])\n",
    "tensor_sum = Lambda(lambda x: K.sum(x, axis = 1), name = 'sum_of_tensors')\n",
    "tensor_square = Lambda(lambda x: K.square(x), name = 'square_of_tensors')\n",
    "\n",
    "sum_of_embed = tensor_sum(embed_2d)\n",
    "square_of_embed = tensor_square(embed_2d)\n",
    "\n",
    "square_of_sum = Multiply()([sum_of_embed, sum_of_embed])\n",
    "sum_of_square = tensor_sum(square_of_embed)\n",
    "\n",
    "sub = Subtract()([square_of_sum, sum_of_square])\n",
    "sub = Lambda(lambda x: x*0.5)(sub)\n",
    "y_fm_2d = Reshape((1,), name = 'fm_2d_output')(tensor_sum(sub))\n",
    "```\n",
    "![title](./image/fm_model_2d.png)\n",
    "\n",
    "* deep part\n",
    "\n",
    "```python\n",
    "# flat embed layers from 3D to 2D tensors\n",
    "y_dnn = Flatten(name = 'flat_embed_2d')(embed_2d)\n",
    "for h in dnn_dim:\n",
    "    y_dnn = Dropout(dnn_dr)(y_dnn)\n",
    "    y_dnn = Dense(h, activation='relu')(y_dnn)\n",
    "y_dnn = Dense(1, activation='relu', name = 'deep_output')(y_dnn)\n",
    "\n",
    "# combinded deep and fm parts\n",
    "y = Concatenate()([y_fm_1d, y_fm_2d, y_dnn])\n",
    "y = Dense(1, name = 'deepfm_output')(y)\n",
    "\n",
    "fm_model_1d = Model(inputs, y_fm_1d)\n",
    "fm_model_2d = Model(inputs, y_fm_2d)\n",
    "deep_model = Model(inputs, y_dnn)\n",
    "deep_fm_model = Model(inputs, y)\n",
    "```\n",
    "![title](./image/deep_model.png)\n",
    "\n",
    "put together all parts:\n",
    "\n",
    "![title](./image/deep_fm_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "def deep_fm_model(n_uid, n_mid, n_genre, k, dnn_dim, dnn_dr):\n",
    "    # numerica features\n",
    "    fea4_input = Input((1,), name = 'input_fea4')\n",
    "    num_inputs = [fea4_input]\n",
    "    # single level categorical features\n",
    "    uid_input = Input((1,), name = 'input_uid')\n",
    "    mid_input = Input((1,), name= 'input_mid')\n",
    "    cat_sl_inputs = [uid_input, mid_input]\n",
    "\n",
    "    # multi level categorical features (with 3 genres at most)\n",
    "    genre_input = Input((3,), name = 'input_genre')\n",
    "    cat_ml_inputs = [genre_input]\n",
    "\n",
    "    inputs = num_inputs + cat_sl_inputs + cat_ml_inputs\n",
    "\n",
    "    # first order fm\n",
    "    # all tensors are reshape to (None, 1)\n",
    "    num_dense_1d = [Dense(1, name = 'num_dense_1d_fea4')(fea4_input)]\n",
    "    cat_sl_embed_1d = [Embedding(n_uid + 1, 1, name = 'cat_embed_1d_uid')(uid_input),\n",
    "                        Embedding(n_mid + 1, 1, name = 'cat_embed_1d_mid')(mid_input)]\n",
    "    cat_ml_embed_1d = [Embedding(n_genre + 1, 1, name = 'cat_embed_1d_genre')(genre_input)]\n",
    "\n",
    "    cat_sl_embed_1d = [Reshape((1,))(i) for i in cat_sl_embed_1d]\n",
    "    cat_ml_embed_1d = [Flatten()(i) for i in cat_ml_embed_1d]\n",
    "\n",
    "    # add all tensors\n",
    "    y_fm_1d = Add(name = 'fm_1d_output')(num_dense_1d + cat_sl_embed_1d + cat_ml_embed_1d)\n",
    "\n",
    "\n",
    "    # second order fm\n",
    "    # reshape all tensors to (None, k)\n",
    "    num_dense_2d = [Dense(k, name = 'num_dense_2d_fea4')(fea4_input)]\n",
    "    cat_sl_embed_2d = [Embedding(n_uid + 1, k, name = 'cat_embed_2d_uid')(uid_input), \n",
    "                       Embedding(n_mid + 1, k, name = 'cat_embed_2d_mid')(mid_input)]\n",
    "    cat_ml_embed_2d = [Embedding(n_genre + 1, k, name = 'cat_embed_2d_genre')(genre_input)]\n",
    "    cat_ml_embed_2d = [Lambda(lambda x: K.mean(x, axis=1), name = 'embed_2d_mean')(i) for i in cat_ml_embed_2d]\n",
    "\n",
    "    num_dense_2d = [Reshape((1,k))(i) for i in num_dense_2d]\n",
    "    cat_ml_embed_2d = [Reshape((1,k))(i) for i in cat_ml_embed_2d]\n",
    "\n",
    "    embed_2d = Concatenate(axis=1, name = 'concat_embed_2d')(num_dense_2d + cat_sl_embed_2d + cat_ml_embed_2d)\n",
    "\n",
    "    # calcuate the interactions by simplication\n",
    "    # sum of (x1*x2) = sum of (0.5*[(xi)^2 - (xi^2)])\n",
    "    tensor_sum = Lambda(lambda x: K.sum(x, axis = 1), name = 'sum_of_tensors')\n",
    "    tensor_square = Lambda(lambda x: K.square(x), name = 'square_of_tensors')\n",
    "\n",
    "    sum_of_embed = tensor_sum(embed_2d)\n",
    "    square_of_embed = tensor_square(embed_2d)\n",
    "\n",
    "    square_of_sum = Multiply()([sum_of_embed, sum_of_embed])\n",
    "    sum_of_square = tensor_sum(square_of_embed)\n",
    "\n",
    "    sub = Subtract()([square_of_sum, sum_of_square])\n",
    "    sub = Lambda(lambda x: x*0.5)(sub)\n",
    "    y_fm_2d = Reshape((1,), name = 'fm_2d_output')(tensor_sum(sub))\n",
    "\n",
    "\n",
    "    # dnn part\n",
    "    y_dnn = Flatten(name = 'flat_embed_2d')(embed_2d)\n",
    "    for h in dnn_dim:\n",
    "        y_dnn = Dropout(dnn_dr)(y_dnn)\n",
    "        y_dnn = Dense(h, activation='relu')(y_dnn)\n",
    "    y_dnn = Dense(1, activation='relu', name = 'deep_output')(y_dnn)\n",
    "\n",
    "    # combinded deep and fm parts\n",
    "    y = Concatenate()([y_fm_1d, y_fm_2d, y_dnn])\n",
    "    y = Dense(1, name = 'deepfm_output')(y)\n",
    "\n",
    "    fm_model_1d = Model(inputs, y_fm_1d)\n",
    "    fm_model_2d = Model(inputs, y_fm_2d)\n",
    "    deep_model = Model(inputs, y_dnn)\n",
    "    deep_fm_model = Model(inputs, y)\n",
    "    \n",
    "    return fm_model_1d, fm_model_2d, deep_model, deep_fm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_uid': ratings.uid.max(),\n",
    "    'n_mid': ratings.mid.max(),\n",
    "    'n_genre': 14,\n",
    "    'k':20,\n",
    "    'dnn_dim':[64,64],\n",
    "    'dnn_dr': 0.5\n",
    "}\n",
    "\n",
    "fm_model_1d, fm_model_2d, deep_model, deep_fm_model = deep_fm_model(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2xy(ratings):\n",
    "    x = [ratings.user_fea3.values, \n",
    "         ratings.uid.values, \n",
    "         ratings.mid.values, \n",
    "         np.concatenate(ratings.movie_genre.values).reshape(-1,3)]\n",
    "    y = ratings.rating.values\n",
    "    return x,y\n",
    "\n",
    "in_train_flag = np.random.random(len(ratings)) <= 0.9\n",
    "train_data = ratings.loc[in_train_flag,]\n",
    "valid_data = ratings.loc[~in_train_flag,]\n",
    "train_x, train_y = df2xy(train_data)\n",
    "valid_x, valid_y = df2xy(valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liuchr/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 810082 samples, validate on 90010 samples\n",
      "Epoch 1/1\n",
      "810082/810082 [==============================] - 20s 25us/step - loss: 8.2028 - val_loss: 9.6406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9eeaa83978>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "# train  model\n",
    "deep_fm_model.compile(loss = 'MSE', optimizer='adam')\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "callbacks = [early_stop]\n",
    "deep_fm_model.fit(train_x, train_y, epochs=30, batch_size=2048, validation_split=0.1, callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(fm_model_1d, to_file='./image/fm_model_1d.png',show_shapes=True, show_layer_names=True)\n",
    "plot_model(fm_model_2d, to_file='./image/fm_model_2d.png',show_shapes=True, show_layer_names=True)\n",
    "plot_model(deep_model, to_file='./image/deep_model.png',show_shapes=True, show_layer_names=True)\n",
    "plot_model(deep_fm_model, to_file='./image/deep_fm_model.png',show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
